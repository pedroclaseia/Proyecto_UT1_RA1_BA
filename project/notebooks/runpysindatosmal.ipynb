{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "# Rutas base\n",
    "ROOT = Path(__file__).resolve().parents[1]\n",
    "DATA = ROOT / \"data\" / \"drops\"\n",
    "OUT = ROOT / \"output\"\n",
    "PARQUET_DIR = OUT / \"parquet\"\n",
    "QUALITY_DIR = OUT / \"quality\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "QUALITY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DB = OUT / \"ut1.db\"\n",
    "\n",
    "# Utilidades\n",
    "def to_float_money(x):\n",
    "    try:\n",
    "        return float(str(x).replace(\",\", \".\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def strip_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_object_dtype(df[c]):\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def classify_file(fname: str) -> str | None:\n",
    "    n = fname.lower()\n",
    "    if any(k in n for k in [\"ventas\", \"venta\"]):\n",
    "        return \"ventas\"\n",
    "    if any(k in n for k in [\"clientes\", \"cliente\"]):\n",
    "        return \"clientes\"\n",
    "    if any(k in n for k in [\"productos\", \"producto\"]):\n",
    "        return \"productos\"\n",
    "    return None\n",
    "\n",
    "# Escritura parquet con aviso si falta engine\n",
    "def write_parquet(df: pd.DataFrame, path: Path, label: str):\n",
    "    try:\n",
    "        df.to_parquet(path, index=False)\n",
    "        print(f\"Parquet escrito: {path.name} ({len(df)} filas) para {label}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"[AVISO] No se pudo escribir {path.name} (instala 'pyarrow' o 'fastparquet'): {e}\")\n",
    "\n",
    "# Cuarentena unificada: a tabla y a CSV único por dominio\n",
    "def append_quarantine(con: sqlite3.Connection, kind: str, reasons_rows: list[tuple[str, str, str, str, str]]):\n",
    "    \"\"\"\n",
    "    reasons_rows: lista de tuplas (_reason, _row, _ingest_ts, _source_file, _batch_id)\n",
    "    Escribe en tabla quarantine_{kind} y en QUALITY_DIR/{kind}_quarantine.csv (append).\n",
    "    \"\"\"\n",
    "    if not reasons_rows:\n",
    "        return\n",
    "    dfq = pd.DataFrame(reasons_rows, columns=[\"_reason\", \"_row\", \"_ingest_ts\", \"_source_file\", \"_batch_id\"])\n",
    "    table = f\"quarantine_{kind}\"\n",
    "    dfq.to_sql(table, con, if_exists=\"append\", index=False)\n",
    "    out_csv = QUALITY_DIR / f\"{kind}_quarantine.csv\"\n",
    "    mode = \"a\" if out_csv.exists() else \"w\"\n",
    "    dfq.to_csv(out_csv, index=False, mode=mode, header=not out_csv.exists())\n",
    "\n",
    "# Separador de líneas bien/mal formadas por conteo de comas\n",
    "def split_good_bad_lines(f: Path) -> tuple[list[str], list[str]]:\n",
    "    raw = f.read_text(encoding=\"utf-8\").splitlines()\n",
    "    if not raw:\n",
    "        return [], []\n",
    "    header = raw[0]\n",
    "    expected_cols = header.count(\",\") + 1\n",
    "    good = [header]\n",
    "    bad = []\n",
    "    for line in raw[1:]:\n",
    "        cols = line.count(\",\") + 1\n",
    "        if cols == expected_cols and line.strip():\n",
    "            good.append(line)\n",
    "        else:\n",
    "            bad.append(line)\n",
    "    return good, bad\n",
    "\n",
    "# Ingesta robusta por fichero con cuarentena de parseo\n",
    "def ingest_one(f: Path, con: sqlite3.Connection, kind: str) -> pd.DataFrame:\n",
    "    batch_id = f.stem.lower()\n",
    "    good_lines, bad_lines = split_good_bad_lines(f)\n",
    "    # Malformadas -> cuarentena unificada\n",
    "    if bad_lines:\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        rows = [(\"parse_error_bad_field_count\", bl, now, f.name, batch_id) for bl in bad_lines]\n",
    "        append_quarantine(con, kind, rows)\n",
    "    if len(good_lines) <= 1:\n",
    "        return pd.DataFrame()\n",
    "    buf = StringIO(\"\\n\".join(good_lines))\n",
    "    df = pd.read_csv(buf, dtype=str, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    df = strip_strings(df)\n",
    "    if \"fecha_venta\" in df.columns:  # ventas -> normaliza a 'fecha'\n",
    "        df = df.rename(columns={\"fecha_venta\": \"fecha\"})\n",
    "    df[\"_source_file\"] = f.name\n",
    "    df[\"_ingest_ts\"] = datetime.now(timezone.utc).isoformat()\n",
    "    df[\"_batch_id\"] = batch_id\n",
    "    return df\n",
    "\n",
    "def ingest_all_csvs_to_raw(con: sqlite3.Connection) -> dict:\n",
    "    counters = {\"ventas\": 0, \"clientes\": 0, \"productos\": 0}\n",
    "    detected = sorted(DATA.glob(\"*.csv\"))\n",
    "    print(\"CSV detectados:\", [p.name for p in detected])\n",
    "    for f in detected:\n",
    "        kind = classify_file(f.name)\n",
    "        if not kind:\n",
    "            print(\"Ignorado (sin match):\", f.name)\n",
    "            continue\n",
    "        df = ingest_one(f, con, kind)\n",
    "        stem = f.stem.lower()\n",
    "\n",
    "        if kind == \"ventas\":\n",
    "            needed = [\"fecha\", \"id_cliente\", \"id_producto\", \"unidades\", \"precio_unitario\", \"_ingest_ts\", \"_source_file\", \"_batch_id\"]\n",
    "            for c in needed:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = None\n",
    "            df_raw = df[needed].copy()\n",
    "            if not df_raw.empty:\n",
    "                df_raw.to_sql(\"raw_ventas\", con, if_exists=\"append\", index=False)\n",
    "                counters[\"ventas\"] += len(df_raw)\n",
    "\n",
    "        elif kind == \"clientes\":\n",
    "            cols = [\"fecha\", \"nombre\", \"apellido\", \"id_cliente\"]\n",
    "            for c in cols:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = None\n",
    "            df_raw = df[cols + [\"_ig_ts\" if \"_ingest_ts\" not in df.columns else \"_ingest_ts\", \"_source_file\", \"_batch_id\"]].copy()\n",
    "            if \"_ig_ts\" in df_raw.columns:\n",
    "                df_raw = df_raw.rename(columns={\"_ig_ts\": \"_ingest_ts\"})\n",
    "            if not df_raw.empty:\n",
    "                df_raw.to_sql(\"raw_clientes\", con, if_exists=\"append\", index=False)\n",
    "                counters[\"clientes\"] += len(df_raw)\n",
    "\n",
    "        elif kind == \"productos\":\n",
    "            cols = [\"fecha_entrada\", \"nombre_producto\", \"id_producto\", \"unidades\", \"precio_unitario\", \"categoria\"]\n",
    "            for c in cols:\n",
    "                if c not in df.columns:\n",
    "                    df[c] = None\n",
    "            df_raw = df[cols + [\"_ingest_ts\", \"_source_file\", \"_batch_id\"]].copy()\n",
    "            if not df_raw.empty:\n",
    "                df_raw.to_sql(\"raw_productos\", con, if_exists=\"append\", index=False)\n",
    "                counters[\"productos\"] += len(df_raw)\n",
    "    return counters\n",
    "\n",
    "# Cargas UPSERT desde sql/10_upserts.sql (una sentencia por tabla)\n",
    "def load_upsert_sqls(path: Path) -> dict[str, str]:\n",
    "    raw = path.read_text(encoding=\"utf-8\").replace(\"\\ufeff\", \"\")\n",
    "    # Quitar comentarios '--'\n",
    "    no_comments = []\n",
    "    for line in raw.splitlines():\n",
    "        line = line.split(\"--\", 1)[0]\n",
    "        if line.strip():\n",
    "            no_comments.append(line)\n",
    "    txt = \"\\n\".join(no_comments)\n",
    "\n",
    "    def extract_one(table: str) -> str:\n",
    "        m = re.search(rf\"(?is)\\binsert\\s+into\\s+{table}\\b\", txt)\n",
    "        if not m:\n",
    "            raise ValueError(f\"No se encontró INSERT INTO {table} en {path.name}\")\n",
    "        after = txt[m.start():]\n",
    "        semi = after.find(\";\")\n",
    "        if semi == -1:\n",
    "            raise ValueError(f\"La sentencia INSERT de {table} no termina en ';' en {path.name}\")\n",
    "        stmt = after[:semi].strip()\n",
    "        if \"values\" not in stmt.lower() or \"on conflict\" not in stmt.lower():\n",
    "            raise ValueError(f\"INSERT de {table} incompleto en {path.name}\")\n",
    "        if \"*\" in stmt:\n",
    "            raise ValueError(f\"INSERT de {table} contiene '*', revisa {path.name}\")\n",
    "        return stmt\n",
    "\n",
    "    return {\n",
    "        \"clean_ventas\": extract_one(\"clean_ventas\"),\n",
    "        \"clean_clientes\": extract_one(\"clean_clientes\"),\n",
    "        \"clean_productos\": extract_one(\"clean_productos\"),\n",
    "    }\n",
    "\n",
    "# Validaciones adicionales para CLIENTES\n",
    "NAME_RE = re.compile(r\"^[A-Za-zÁÉÍÓÚÜÑáéíóúüñ\\s'-]+$\")\n",
    "\n",
    "def validate_clientes(df: pd.DataFrame) -> pd.Series:\n",
    "    # Reglas: fecha válida, nombre/apellido solo letras/espacios, id_cliente formato CNNN\n",
    "    fecha_ok = pd.to_datetime(df[\"fecha\"], errors=\"coerce\").notna()\n",
    "    nombre_ok = df[\"nombre\"].fillna(\"\").str.len().gt(0) & df[\"nombre\"].fillna(\"\").str.match(NAME_RE)\n",
    "    apellido_ok = df[\"apellido\"].fillna(\"\").str.len().gt(0) & df[\"apellido\"].fillna(\"\").str.match(NAME_RE)\n",
    "    id_norm = df[\"id_cliente\"].fillna(\"\").str.upper().str.strip()\n",
    "    id_ok = id_norm.str.match(r\"^C\\d{3}$\")\n",
    "    return fecha_ok & nombre_ok & apellido_ok & id_ok\n",
    "\n",
    "def serialize_row_csv_like(row: pd.Series, cols: list[str]) -> str:\n",
    "    # Representación simple CSV-like para _row de cuarentena\n",
    "    values = []\n",
    "    for c in cols:\n",
    "        v = row.get(c, \"\")\n",
    "        if v is None:\n",
    "            v = \"\"\n",
    "        s = str(v)\n",
    "        if \",\" in s or '\"' in s:\n",
    "            s = '\"' + s.replace('\"', '\"\"') + '\"'\n",
    "        values.append(s)\n",
    "    return \",\".join(values)\n",
    "\n",
    "# Limpieza VENTAS\n",
    "def clean_and_persist_ventas_from_raw(con: sqlite3.Connection, upsert_sql: str) -> tuple[int, int, int]:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM raw_ventas\", con)\n",
    "    raw_rows = len(df)\n",
    "    if df.empty:\n",
    "        (QUALITY_DIR / \"ventas_quarantine.csv\").touch(exist_ok=True)\n",
    "        return 0, 0, 0\n",
    "\n",
    "    df = strip_strings(df)\n",
    "    for c in [\"fecha\", \"id_cliente\", \"id_producto\", \"unidades\", \"precio_unitario\", \"_ingest_ts\", \"_source_file\", \"_batch_id\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\").dt.date\n",
    "    df[\"unidades\"] = pd.to_numeric(df[\"unidades\"], errors=\"coerce\")\n",
    "    df[\"precio_unitario\"] = df[\"precio_unitario\"].apply(to_float_money)\n",
    "\n",
    "    valid = (\n",
    "        pd.notna(df[\"fecha\"])\n",
    "        & df[\"unidades\"].notna() & (df[\"unidades\"] >= 0)\n",
    "        & df[\"precio_unitario\"].notna() & (df[\"precio_unitario\"] >= 0)\n",
    "        & df[\"id_cliente\"].fillna(\"\").ne(\"\")\n",
    "        & df[\"id_producto\"].fillna(\"\").ne(\"\")\n",
    "    )\n",
    "    quarantine = df.loc[~valid].copy()\n",
    "    clean = df.loc[valid].copy()\n",
    "\n",
    "    # Cuarentena unificada para inválidas\n",
    "    if not quarantine.empty:\n",
    "        cols_src = [\"fecha\", \"id_cliente\", \"id_producto\", \"unidades\", \"precio_unitario\"]\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        reasons_rows = []\n",
    "        for _, r in quarantine.iterrows():\n",
    "            reason = \"validation_failed\"\n",
    "            row_text = serialize_row_csv_like(r, cols_src)\n",
    "            reasons_rows.append((reason, row_text, now, r.get(\"_source_file\", \"\"), r.get(\"_batch_id\", \"\")))\n",
    "        append_quarantine(con, \"ventas\", reasons_rows)\n",
    "\n",
    "    if not clean.empty:\n",
    "        clean = clean.sort_values(\"_ingest_ts\").drop_duplicates(\n",
    "            subset=[\"fecha\", \"id_cliente\", \"id_producto\"], keep=\"last\"\n",
    "        )\n",
    "        write_parquet(clean, PARQUET_DIR / \"clean_ventas.parquet\", \"ventas\")\n",
    "        for _, r in clean.iterrows():\n",
    "            con.execute(\n",
    "                upsert_sql,\n",
    "                {\n",
    "                    \"fecha\": str(r[\"fecha\"]),\n",
    "                    \"idc\": r[\"id_cliente\"],\n",
    "                    \"idp\": r[\"id_producto\"],\n",
    "                    \"u\": float(r[\"unidades\"]),\n",
    "                    \"p\": float(r[\"precio_unitario\"]),\n",
    "                    \"ts\": r[\"_ingest_ts\"],\n",
    "                },\n",
    "            )\n",
    "        con.commit()\n",
    "\n",
    "    return raw_rows, len(clean), len(quarantine)\n",
    "\n",
    "# Limpieza CLIENTES con reglas nuevas\n",
    "def clean_and_persist_clientes_from_raw(con: sqlite3.Connection, upsert_sql: str) -> tuple[int, int, int]:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM raw_clientes\", con)\n",
    "    raw_rows = len(df)\n",
    "    if df.empty:\n",
    "        (QUALITY_DIR / \"clientes_quarantine.csv\").touch(exist_ok=True)\n",
    "        return 0, 0, 0\n",
    "\n",
    "    df = strip_strings(df)\n",
    "    for c in [\"fecha\", \"nombre\", \"apellido\", \"id_cliente\", \"_ingest_ts\", \"_source_file\", \"_batch_id\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    valid = validate_clientes(df)\n",
    "    quarantine = df.loc[~valid].copy()\n",
    "    clean = df.loc[valid].copy()\n",
    "\n",
    "    # Cuarentena unificada para inválidas de clientes\n",
    "    if not quarantine.empty:\n",
    "        cols_src = [\"fecha\", \"nombre\", \"apellido\", \"id_cliente\"]\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        reasons_rows = []\n",
    "        for _, r in quarantine.iterrows():\n",
    "            reason = \"validation_failed_clientes\"\n",
    "            row_text = serialize_row_csv_like(r, cols_src)\n",
    "            reasons_rows.append((reason, row_text, now, r.get(\"_source_file\", \"\"), r.get(\"_batch_id\", \"\")))\n",
    "        append_quarantine(con, \"clientes\", reasons_rows)\n",
    "\n",
    "    if not clean.empty:\n",
    "        # last-wins por _ingest_ts\n",
    "        clean = clean.sort_values(\"_ingest_ts\").drop_duplicates(subset=[\"id_cliente\"], keep=\"last\")\n",
    "        write_parquet(clean[[\"id_cliente\", \"nombre\", \"apellido\", \"fecha\"]], PARQUET_DIR / \"clean_clientes.parquet\", \"clientes\")\n",
    "        # UPSERT desde SQL\n",
    "        for _, r in clean.iterrows():\n",
    "            con.execute(\n",
    "                upsert_sql,\n",
    "                {\n",
    "                    \"fecha\": str(r[\"fecha\"]) if pd.notna(r[\"fecha\"]) else None,\n",
    "                    \"nombre\": r[\"nombre\"],\n",
    "                    \"apellido\": r[\"apellido\"],\n",
    "                    \"idc\": r[\"id_cliente\"].upper().strip(),\n",
    "                    \"ts\": r[\"_ingest_ts\"],\n",
    "                },\n",
    "            )\n",
    "        con.commit()\n",
    "\n",
    "    return raw_rows, len(clean), len(quarantine)\n",
    "\n",
    "# Limpieza PRODUCTOS (cuarentena unificada)\n",
    "def clean_and_persist_productos_from_raw(con: sqlite3.Connection, upsert_sql: str) -> tuple[int, int, int]:\n",
    "    df = pd.read_sql_query(\"SELECT * FROM raw_productos\", con)\n",
    "    raw_rows = len(df)\n",
    "    if df.empty:\n",
    "        (QUALITY_DIR / \"productos_quarantine.csv\").touch(exist_ok=True)\n",
    "        return 0, 0, 0\n",
    "\n",
    "    df = strip_strings(df)\n",
    "    for c in [\"fecha_entrada\", \"nombre_producto\", \"id_producto\", \"unidades\", \"precio_unitario\", \"categoria\", \"_ingest_ts\", \"_source_file\", \"_batch_id\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    df[\"fecha_entrada\"] = pd.to_datetime(df[\"fecha_entrada\"], errors=\"coerce\").dt.date\n",
    "    df[\"unidades\"] = pd.to_numeric(df[\"unidades\"], errors=\"coerce\")\n",
    "    df[\"precio_unitario\"] = df[\"precio_unitario\"].apply(to_float_money)\n",
    "\n",
    "    valid = (\n",
    "        df[\"id_producto\"].fillna(\"\").ne(\"\")\n",
    "        & df[\"precio_unitario\"].notna() & (df[\"precio_unitario\"] >= 0)\n",
    "        & df[\"unidades\"].notna() & (df[\"unidades\"] >= 0)\n",
    "    )\n",
    "    quarantine = df.loc[~valid].copy()\n",
    "    clean = df.loc[valid].copy()\n",
    "\n",
    "    # Cuarentena unificada para inválidas de productos\n",
    "    if not quarantine.empty:\n",
    "        cols_src = [\"fecha_entrada\", \"nombre_producto\", \"id_producto\", \"unidades\", \"precio_unitario\", \"categoria\"]\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        reasons_rows = []\n",
    "        for _, r in quarantine.iterrows():\n",
    "            reason = \"validation_failed\"\n",
    "            row_text = serialize_row_csv_like(r, cols_src)\n",
    "            reasons_rows.append((reason, row_text, now, r.get(\"_source_file\", \"\"), r.get(\"_batch_id\", \"\")))\n",
    "        append_quarantine(con, \"productos\", reasons_rows)\n",
    "\n",
    "    if not clean.empty:\n",
    "        clean = clean.sort_values(\"_ingest_ts\").drop_duplicates(subset=[\"id_producto\"], keep=\"last\")\n",
    "        write_parquet(\n",
    "            clean[[\"id_producto\", \"nombre_producto\", \"categoria\", \"precio_unitario\", \"unidades\", \"fecha_entrada\"]],\n",
    "            PARQUET_DIR / \"clean_productos.parquet\",\n",
    "            \"productos\",\n",
    "        )\n",
    "        for _, r in clean.iterrows():\n",
    "            con.execute(\n",
    "                upsert_sql,\n",
    "                {\n",
    "                    \"fecha_entrada\": str(r[\"fecha_entrada\"]) if pd.notna(r[\"fecha_entrada\"]) else None,\n",
    "                    \"nombre_producto\": r[\"nombre_producto\"],\n",
    "                    \"idp\": r[\"id_producto\"],\n",
    "                    \"u\": float(r[\"unidades\"]),\n",
    "                    \"p\": float(r[\"precio_unitario\"]),\n",
    "                    \"cat\": r[\"categoria\"],\n",
    "                    \"ts\": r[\"_ingest_ts\"],\n",
    "                },\n",
    "            )\n",
    "        con.commit()\n",
    "\n",
    "    return raw_rows, len(clean), len(quarantine)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    con = sqlite3.connect(DB)\n",
    "    try:\n",
    "        print(\"DB path:\", (OUT / \"ut1.db\").resolve())\n",
    "\n",
    "        # 1) Esquema desde archivo\n",
    "        con.executescript((ROOT / \"sql\" / \"00_schema.sql\").read_text(encoding=\"utf-8\"))\n",
    "        con.commit()\n",
    "        print(\"Tablas tras esquema:\", con.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\").fetchall())\n",
    "\n",
    "        # 2) Ingesta RAW con cuarentena unificada de parseo\n",
    "        counters = ingest_all_csvs_to_raw(con)\n",
    "        con.commit()\n",
    "        print(\"RAW counters:\", counters)\n",
    "        print(\n",
    "            \"RAW counts:\",\n",
    "            con.execute(\"SELECT COUNT(*) FROM raw_ventas\").fetchone()[0],\n",
    "            con.execute(\"SELECT COUNT(*) FROM raw_clientes\").fetchone()[0],\n",
    "            con.execute(\"SELECT COUNT(*) FROM raw_productos\").fetchone()[0],\n",
    "        )\n",
    "\n",
    "        # 3) UPSERTs desde sql/10_upserts.sql\n",
    "        upserts = load_upsert_sqls(ROOT / \"sql\" / \"10_upserts.sql\")\n",
    "\n",
    "        # 4) Limpieza + persistencia + parquet + cuarentena unificada\n",
    "        rv = clean_and_persist_ventas_from_raw(con, upserts[\"clean_ventas\"])\n",
    "        rc = clean_and_persist_clientes_from_raw(con, upserts[\"clean_clientes\"])\n",
    "        rp = clean_and_persist_productos_from_raw(con, upserts[\"clean_productos\"])\n",
    "        print(\"Ventas (raw, clean, quar):\", rv)\n",
    "        print(\"Clientes (raw, clean, quar):\", rc)\n",
    "        print(\"Productos (raw, clean, quar):\", rp)\n",
    "\n",
    "        # 5) Vistas\n",
    "        con.executescript((ROOT / \"sql\" / \"20_views.sql\").read_text(encoding=\"utf-8\"))\n",
    "        con.commit()\n",
    "        print(\"Vistas finales:\", con.execute(\"SELECT name FROM sqlite_master WHERE type='view' ORDER BY name;\").fetchall())\n",
    "    finally:\n",
    "        con.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
